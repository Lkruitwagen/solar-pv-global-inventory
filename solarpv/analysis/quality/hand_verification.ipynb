{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import descarteslabs as dl\n",
    "import h5py\n",
    "import requests\n",
    "from shapely import geometry\n",
    "from geojson import Feature, FeatureCollection\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import geojson\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "from area import area\n",
    "import datetime\n",
    "from pyproj import Proj\n",
    "import random\n",
    "import glob\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.misc import factorial\n",
    "\n",
    "from functools import partial\n",
    "from shapely.ops import transform\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "\n",
    "from shapely.ops import transform,linemerge, unary_union, polygonize\n",
    "from shapely.affinity import affine_transform\n",
    "from functools import partial\n",
    "import pyproj\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "from shp_utils_py3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Images for Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_client = dl.Catalog()\n",
    "raster_client = dl.Raster()\n",
    "metadata_client = dl.Metadata()\n",
    "tasks = dl.Tasks()\n",
    "vector_client = dl.Vector()\n",
    "storage_client = dl.client.services.storage.Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '<your-api-key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'FR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./hand_verify_B/'+dir_name+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_str='US-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob('./results_fcs_additional/'+'FR'+'*.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [f for f in fnames if not ('AU' in f or 'BR' in f) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fts = []\n",
    "for f in fnames:\n",
    "    run_fts += json.load(open(f,'r'))['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fts = [ft for ft in run_fts if ft['properties']['confidence']=='B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(run_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_poly(ft_poly,mmpix):\n",
    "\n",
    "    #V_inv -> lat,lon\n",
    "    #for each point, go centroid->pt\n",
    "    #print(centroid)\n",
    "    img_coords = []\n",
    "    for pt in list(ft_poly.exterior.coords):\n",
    "        #pt->lon,lat\n",
    "        #centroid->lon,lat\n",
    "        dist, angle, dummy = V_inv((ft_poly.centroid.y,ft_poly.centroid.x),(pt[1],pt[0]))\n",
    "        dist=dist*1000\n",
    "        #print ((400+2*(dist/mmpix*np.cos(2.*np.pi*(angle)/360.)),400+2*(dist/mmpix*np.sin(2*np.pi*angle/360.))))\n",
    "        img_coords.append((400+2*(dist/mmpix*np.cos(2.*np.pi*(angle-90.)/360.)),400+2*(dist/mmpix*np.sin(2*np.pi*(angle-90.)/360.))))\n",
    "    #V_inv(point1, point2\n",
    "    #v_dir\n",
    "    return img_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lonlat2pixXY(pt,dt):\n",
    "    lon = pt[0]\n",
    "    lat = pt[1]\n",
    "    Y = (lat-dt[3]-dt[4]/dt[1]*(lon-dt[0]))/(dt[5]-dt[2]*dt[4]/dt[1])\n",
    "    #print Y\n",
    "    X = (lon-dt[0]-Y*dt[2])/dt[1]\n",
    "    #print (lon,dt[0],X)\n",
    "    #print X\n",
    "    return [int(X),int(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_sample(ft,src = 'google'):\n",
    "    \n",
    "    print (ft['properties'])\n",
    "    ft_shp = geometry.shape(ft['geometry'])\n",
    "    #print ('area',))\n",
    "    print(ft_shp.type)\n",
    "    if ft_shp.type=='MultiPolygon':\n",
    "        run_polys = list(ft_shp)\n",
    "    else:\n",
    "        run_polys = [ft_shp]\n",
    "        \n",
    "    for ii_p,ft_poly in enumerate(run_polys):\n",
    "        centroid = ft_poly.centroid ##lon,lat\n",
    "        print ('centroid',centroid)\n",
    "        ft_bbox = ft_poly.bounds\n",
    "        \n",
    "        Dx = ft_poly.bounds[2]-ft_poly.bounds[0]\n",
    "        Dy = ft_poly.bounds[3]-ft_poly.bounds[1]\n",
    "        print (max(abs(Dx),abs(Dy)))\n",
    "        ft_box = geometry.box(*ft_poly.buffer(max(abs(Dx),abs(Dy))).bounds)\n",
    "        \n",
    "        \n",
    "        fig, axs=plt.subplots(1,2,figsize=(24,12))\n",
    "\n",
    "        if src=='google':\n",
    "            box_sides = (V_inv((ft_bbox[1],ft_bbox[0]),(ft_bbox[1],ft_bbox[2]))[0]*1000,\n",
    "                             V_inv((ft_bbox[1],ft_bbox[0]),(ft_bbox[3],ft_bbox[0]))[0]*1000)\n",
    "            #print (box_sides)\n",
    "            side_len = np.ceil(max(box_sides))\n",
    "            print ('side_len (m)',side_len)\n",
    "\n",
    "            zoom_dict = dict(zip(range(1,21),[156543.03392 * np.cos(centroid.xy[1][0] * np.pi / 180) / np.power(2, z) for z in range(1,21)]))\n",
    "            #print (zoom_dict) ##<-- METERS PER PIX, not side length\n",
    "\n",
    "            zoom = np.max(np.argwhere(np.array([(zoom_dict[k]*400-max(box_sides)) for k in range(1,21)])>0.))+1\n",
    "\n",
    "\n",
    "            #min(zoom_dict.keys(), key=(lambda k: (zoom_dict[k]-max(box_sides))))\n",
    "            print ('zoom',zoom, zoom_dict[zoom], 'area',area(ft['geometry']), np.array(centroid))\n",
    "            pix_poly = img_poly(ft_poly,zoom_dict[zoom])\n",
    "\n",
    "            urlstr = ''.join([\"\"\"https://maps.googleapis.com/maps/api/staticmap?center=\"\"\",\n",
    "                        str(centroid.xy[1][0])+\"\"\",\"\"\"+str(centroid.xy[0][0]),\n",
    "                        \"\"\"&zoom=\"\"\"+str(zoom),\n",
    "                        \"\"\"&size=400x400&scale=2&maptype=satellite&format=png&key=\"\"\", str(API_KEY)])\n",
    "\n",
    "            r = requests.get(urlstr, allow_redirects=True)\n",
    "            #print (r.content)\n",
    "\n",
    "            image_data = r.content\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "            arr = np.asarray(image)\n",
    "            \n",
    "            axs[0].imshow(arr)\n",
    "            xs, ys = geometry.Polygon(pix_poly).exterior.xy\n",
    "            #print (geometry.Polygon(pix_poly))\n",
    "            #print (xs,ys)\n",
    "            axs[0].plot(xs,ys, color='c', linewidth=2.)\n",
    "            axs[0].set_title('Google Basemap (indeterminate date), {:.2f}m/px'.format(zoom_dict[zoom]), fontsize=20)\n",
    "        \n",
    "            \n",
    "        elif src=='SPOT':\n",
    "            scenes, ctx = dl.scenes.search(\n",
    "            ft_box,\n",
    "            products=['airbus:oneatlas:spot:v2'],\n",
    "            start_datetime='2015-09-01',  end_datetime='2018-12-31', cloud_fraction=0.2, limit=10,\n",
    "            )\n",
    "            scenes = sorted(scenes, key=lambda k: k.properties.date, reverse=True)\n",
    "            print ([s.properties.date for s in scenes])\n",
    "            \n",
    "            arr_SPOT = scenes[0].ndarray(\"red green blue\", ctx)\n",
    "\n",
    "            for s in scenes[1:]:\n",
    "                fill_por = np.sum(arr_SPOT>0)/np.prod([*arr_SPOT.shape])\n",
    "                #print (fill_por, np.sum(arr_S2), np.prod([*arr_S2.shape]))\n",
    "                new_arr = s.ndarray(\"red green blue\", ctx)\n",
    "                #print ('shapes',new_arr.shape, np.sum(new_arr>0))\n",
    "\n",
    "                arr_SPOT[new_arr.mask==False]=new_arr.data[new_arr.mask==False]\n",
    "\n",
    "\n",
    "            scene_crs = scenes[0].properties['crs']\n",
    "            dt = scenes[0].properties['geotrans']\n",
    "            dt_shapely = [dt[1],dt[2],dt[4],dt[5],dt[0],dt[3]]\n",
    "\n",
    "            WGS84 = \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n",
    "\n",
    "            wgs_proj = pyproj.Proj(WGS84)\n",
    "            utm_proj = pyproj.Proj(\"+init=\"+scene_crs, preserve_units=True)\n",
    "\n",
    "\n",
    "            dt[0] = utm_proj(*ctx.bounds[0:2])[0]\n",
    "            dt[3] = utm_proj(*ctx.bounds[2:])[1]\n",
    "\n",
    "            projection_func = partial(pyproj.transform, wgs_proj, utm_proj)\n",
    "\n",
    "            utm_poly = transform(projection_func, ft_poly)\n",
    "            pix_poly = geometry.Polygon([lonlat2pixXY(c,dt) for c in list(utm_poly.exterior.coords)])\n",
    "\n",
    "            axs[0].imshow((np.swapaxes(np.swapaxes(arr_SPOT.data,0,2),0,1)/256).clip(0.,1.))\n",
    "            xs,ys = pix_poly.exterior.xy\n",
    "            axs[0].plot(xs,ys,c='c',linewidth=2)\n",
    "            \n",
    "            axs[0].set_title('SPOT {}, 10m/px, , {:d}x{:d}px'.format(scenes[0].properties.date,arr_SPOT.shape[1],arr_SPOT.shape[2]), fontsize=20)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        scenes, ctx = dl.scenes.search(\n",
    "            ft_box,\n",
    "            products=['sentinel-2:L1C'],\n",
    "            start_datetime='2018-09-01',  end_datetime='2018-12-31', cloud_fraction=0.2, limit=10,\n",
    "            )\n",
    "        \n",
    "        if len(scenes)>0:\n",
    "            scenes = sorted(scenes, key=lambda k: k.properties.cloud_fraction, reverse=False)\n",
    "\n",
    "            arr_S2 = scenes[0].ndarray(\"red green blue\", ctx)\n",
    "\n",
    "            for s in scenes[1:]:\n",
    "                fill_por = np.sum(arr_S2>0)/np.prod([*arr_S2.shape])\n",
    "                #print (fill_por, np.sum(arr_S2), np.prod([*arr_S2.shape]))\n",
    "                new_arr = s.ndarray(\"red green blue\", ctx)\n",
    "                #print ('shapes',new_arr.shape, np.sum(new_arr>0))\n",
    "\n",
    "                arr_S2[new_arr.mask==False]=new_arr.data[new_arr.mask==False]\n",
    "\n",
    "\n",
    "            scene_crs = scenes[0].properties['crs']\n",
    "            dt = scenes[0].properties['geotrans']\n",
    "            dt_shapely = [dt[1],dt[2],dt[4],dt[5],dt[0],dt[3]]\n",
    "\n",
    "            WGS84 = \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n",
    "\n",
    "            wgs_proj = pyproj.Proj(WGS84)\n",
    "            utm_proj = pyproj.Proj(\"+init=\"+scene_crs, preserve_units=True)\n",
    "\n",
    "\n",
    "            dt[0] = utm_proj(*ctx.bounds[0:2])[0]\n",
    "            dt[3] = utm_proj(*ctx.bounds[2:])[1]\n",
    "\n",
    "            projection_func = partial(pyproj.transform, wgs_proj, utm_proj)\n",
    "\n",
    "            utm_poly = transform(projection_func, ft_poly)\n",
    "            pix_poly = geometry.Polygon([lonlat2pixXY(c,dt) for c in list(utm_poly.exterior.coords)])\n",
    "\n",
    "            axs[1].imshow((np.swapaxes(np.swapaxes(arr_S2.data,0,2),0,1)/10000*2.5).clip(0.,1.))\n",
    "            xs,ys = pix_poly.exterior.xy\n",
    "            axs[1].plot(xs,ys,c='c',linewidth=2)\n",
    "        axs[1].set_title('Sentinel-2 (Q4 2018), 1.5m/px, , {:d}x{:d}px'.format(arr_S2.shape[1],arr_S2.shape[2]), fontsize=20)\n",
    "        fig.suptitle(ft['properties']['primary_id'], fontsize=26)\n",
    "        #plt.tight_layout()\n",
    "        fig.savefig('./hand_verify_B/'+dir_name+'/'+ft['properties']['primary_id']+'__'+str(ii_p)+'.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in run_fts[3:5]:\n",
    "    inspect_sample(ft,src='SPOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "print ('cpus: ',mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_fts = [(ft,) for ft in run_fts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deploy_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(7)\n",
    "\n",
    "jobs = []\n",
    "#for i in range(5):\n",
    "    \n",
    "\n",
    "#for ii_ft,ft in enumerate(run_fts[0:20]):\n",
    "\n",
    "#    print ('doing ii_ft',ii_ft)\n",
    "#    p = mp.Process(target=inspect_sample, args=[ft,])\n",
    "#    p.start()\n",
    "#    p.join()\n",
    "    \n",
    "for res in pool.starmap(inspect_sample,deploy_fts[655:]):\n",
    "    print (res)\n",
    "    #get_trn_data(k,ii_k, TP_polys, FP_polys)\n",
    "#for res in pool.starmap(fetch_tile_data,list(zip(dltiles['features'], range(len(dltiles['features']))))):\n",
    "#    print (res)\n",
    "    \n",
    "#pickle.dump(res, open('res_test.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter final images for v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dir\n",
    "out_str = 'CL'\n",
    "labels_A = json.load(open('./S2_pics_A/'+out_str+'/labels.json','r')) \n",
    "labels_C = json.load(open('./S2_pics_C/'+out_str+'/labels.json','r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_labels_A = [ll['fname'].split('__')[0].split('/')[-1] for ll in labels_A if ll['label']==0]\n",
    "dn_labels_C = [ll['fname'].split('__')[0].split('/')[-1] for ll in labels_C if ll['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dn_labels_A), len(labels_A), len(dn_labels_C), len(labels_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = sorted(glob.glob('./results_fcs_v1.0/CL*.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fts = []\n",
    "for f in fnames:\n",
    "    all_fts+=json.load(open(f,'r'))['features']\n",
    "    print (f, len(all_fts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_ii=[]\n",
    "for ii_ft,ft in enumerate(all_fts):\n",
    "    if ft['properties']['confidence']=='A':\n",
    "        if ft['properties']['S2_ids_0'] in dn_labels_A:\n",
    "            dn_ii.append(ii_ft)\n",
    "    \n",
    "    elif ft['properties']['confidence']=='C':\n",
    "        if ft['properties']['S2_ids_0'] in dn_labels_C:\n",
    "            dn_ii.append(ii_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dn_ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fts = [ft for ii_ft, ft in enumerate(all_fts) if ii_ft not in dn_ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_fts), len(out_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson.FeatureCollection(out_fts),open('./results_fcs_v1.1/'+out_str+'_v1.1.geojson','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Images for v1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get hand-made labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_str = 'GB'\n",
    "labels_B = json.load(open('./S2_pics_B/'+out_str+'/labels.json','r'))\n",
    "labels_C = json.load(open('./S2_pics_C_gt1/'+out_str+'/labels.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_labels_B = [ll['fname'].split('__')[0].split('/')[-1] for ll in labels_B if ll['label']==1]\n",
    "dn_labels_C = [ll['fname'].split('__')[0].split('/')[-1] for ll in labels_C if ll['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(keep_labels_B), len(labels_B),len(dn_labels_C),len(labels_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out 'B' confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = sorted(glob.glob('./results_fcs_v1.1/GB*.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fts = []\n",
    "for f in fnames:\n",
    "    all_fts+=json.load(open(f,'r'))['features']\n",
    "    print (f, len(all_fts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_ii=[]\n",
    "for ii_ft,ft in enumerate(all_fts):\n",
    "    if ft['properties']['confidence']=='B':\n",
    "        if not ft['properties']['SPOT_ids_0'] in keep_labels_B:\n",
    "            dn_ii.append(ii_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fts = [ft for ii_ft, ft in enumerate(all_fts) if ii_ft not in dn_ii]\n",
    "print (len(out_fts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter additional Cs then add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fs = sorted(glob.glob('./results_fcs_additional/GB*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fts = []\n",
    "for f in add_fs:\n",
    "    add_fts+=json.load(open(f,'r'))['features']\n",
    "    print (f, len(add_fts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fts = [ft for ft in add_fts if ft['properties']['primary_id'] not in dn_labels_C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(add_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in add_fts:\n",
    "    properties = {\n",
    "        'confidence': 'C',\n",
    "        'install_date': ft['properties']['install_date'],\n",
    "        'install_date_ints': ft['properties']['install_date_int'],\n",
    "        'S2_ids_0': ft['properties']['primary_id'],\n",
    "        'SPOT_ids_0': '',\n",
    "        'area': area(json.dumps(ft['geometry']).replace('(','[').replace(')',']'))}\n",
    "    out_fts.append(geojson.Feature(geometry=ft['geometry'],properties=properties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_fts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resmash confidence Bs and Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_treenx(A_polys,P_polys):\n",
    "    ### return a list of matches. both polys need a property which is primary_id\n",
    "    \n",
    "    P_tree = STRtree(P_polys)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for pp_A in A_polys:\n",
    "        t_result = P_tree.query(pp_A)\n",
    "        t_result = [pp for pp in t_result if pp.intersects(pp_A)]\n",
    "        #print (len(t_result))\n",
    "        if len(t_result)>0:\n",
    "            for r in t_result:\n",
    "                G.add_edge(pp_A.primary_id,r.primary_id)\n",
    "                \n",
    "    return list(nx.connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce2mp(polys, verbose=False):\n",
    "    ### generate a big multipolygon -> takes forever. Make small mps of 100 each then, combine.\n",
    "    if len(polys)==1:\n",
    "        return polys[0]\n",
    "    big_mps = []\n",
    "    big_mp = polys[0]\n",
    "    mod_count=0\n",
    "    for ii in range(1,len(polys)):\n",
    "        if ii%100==0:\n",
    "            if verbose:\n",
    "                print ('mod count',ii)\n",
    "            mod_count+=1\n",
    "            big_mps.append(big_mp)\n",
    "            big_mp=polys[ii]\n",
    "        else:\n",
    "            #print (mod_count,ii)\n",
    "            big_mp=big_mp.union(polys[ii])\n",
    "    big_mps.append(big_mp)\n",
    "            \n",
    "    if verbose:\n",
    "        print ('n big mps',len(big_mps))\n",
    "    \n",
    "    ### now reduce list of big_mps\n",
    "    big_mp=big_mps[0]\n",
    "    for ii in range(1,len(big_mps)):\n",
    "        if verbose:\n",
    "            print ('big_mp: ',ii)\n",
    "        big_mp = big_mp.union(big_mps[ii])\n",
    "    return big_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_polys = []\n",
    "for ft in out_fts:\n",
    "    if ft['properties']['confidence']=='B':\n",
    "        pp = geometry.shape(ft['geometry'])\n",
    "        pp.primary_id = ft['properties']['SPOT_ids_0']\n",
    "        pp.install_date = ft['properties']['install_date']\n",
    "        pp.install_date_int = ''\n",
    "        B_polys.append(pp)\n",
    "C_polys = []\n",
    "for ft in out_fts:\n",
    "    if ft['properties']['confidence']=='C':\n",
    "        pp = geometry.shape(ft['geometry'])\n",
    "        pp.primary_id = ft['properties']['S2_ids_0']\n",
    "        pp.install_date = ft['properties']['install_date']\n",
    "        pp.install_date_int = ''\n",
    "        C_polys.append(pp)\n",
    "\n",
    "print (len(B_polys), len(C_polys))\n",
    "matches = get_matches_treenx(C_polys,B_polys)\n",
    "\n",
    "print ('n matches',len(matches))\n",
    "#print (matches)\n",
    "\n",
    "new_A_fts = []\n",
    "done_S2 = []\n",
    "done_SPOT = []\n",
    "\n",
    "\n",
    "for match in matches:\n",
    "    print (match)\n",
    "    m_polys_S2 = [pp for pp in C_polys if pp.primary_id in match]\n",
    "    m_polys_SPOT = [pp for pp in B_polys if pp.primary_id in match]\n",
    "\n",
    "    m_mp_S2 = reduce2mp(m_polys_S2)\n",
    "    m_mp_SPOT = reduce2mp(m_polys_SPOT)\n",
    "\n",
    "    intersection =m_mp_S2.intersection(m_mp_SPOT)\n",
    "    union=m_mp_S2.union(m_mp_SPOT)\n",
    "\n",
    "    IOU = area(json.dumps(geometry.mapping(intersection)).replace('(','[').replace(')',']')) / area(json.dumps(geometry.mapping(union)).replace('(','[').replace(')',']'))\n",
    "\n",
    "    print ('IOU',IOU)\n",
    "\n",
    "    if IOU>0.3:\n",
    "        install_date_ints = ','.join(list(set([str(pp.install_date_int) for pp in m_polys_S2])))\n",
    "        install_dates = ','.join(list(set([pp.install_date for pp in m_polys_S2])))\n",
    "\n",
    "        properties_upload = {\n",
    "                'confidence':'A',\n",
    "                'install_date':install_dates,\n",
    "                'install_date_ints':install_date_ints,\n",
    "            }\n",
    "        S2_ids = [pp.primary_id for pp in m_polys_S2]\n",
    "        SPOT_ids = [pp.primary_id for pp in m_polys_SPOT]\n",
    "\n",
    "\n",
    "        for index in range(int(len(S2_ids)/10)+1):\n",
    "            properties_upload['S2_ids_'+str(index)] = ','.join(S2_ids[index*10:min(((index+1)*10),len(S2_ids))])\n",
    "\n",
    "        for index in range(int(len(SPOT_ids)/7)+1):\n",
    "            properties_upload['SPOT_ids_'+str(index)] = ','.join(SPOT_ids[index*7:min(((index+1)*7),len(SPOT_ids))])          \n",
    "\n",
    "\n",
    "        #print ('install date',install_date_ints,install_dates)\n",
    "        for pp_SPOT in m_polys_SPOT:\n",
    "            properties_upload['area']=area(json.dumps(geometry.mapping(pp_SPOT)).replace('(','[').replace(')',']'))\n",
    "\n",
    "            new_A_fts.append(geojson.Feature(\n",
    "                                geometry=pp_SPOT.simplify(1e-5), \n",
    "                                properties=properties_upload\n",
    "                            ))\n",
    "\n",
    "\n",
    "        done_S2 += S2_ids\n",
    "        done_SPOT += SPOT_ids\n",
    "\n",
    "#add residual Cs\n",
    "out_fts_trim = [ft for ft in out_fts if (ft['properties']['confidence']=='C') and (ft['properties']['S2_ids_0'] not in S2_ids)]\n",
    "print ('resid Cs',len(out_fts_trim))\n",
    "\n",
    "#add residual Bs\n",
    "out_fts_trim += [ft for ft in out_fts if (ft['properties']['confidence']=='B') and (ft['properties']['SPOT_ids_0'] not in SPOT_ids)]\n",
    "print ('residd Bs',len(out_fts_trim))\n",
    "\n",
    "#add new As\n",
    "out_fts_trim+=new_A_fts\n",
    "print ('new As',len(out_fts_trim))\n",
    "\n",
    "#add initial As\n",
    "out_fts_trim+=[ft for ft in out_fts if ft['properties']['confidence']=='A']\n",
    "\n",
    "print ('initial As',len(out_fts_trim), len(out_fts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fix date int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in out_fts_trim:\n",
    "    if ft['properties']['install_date']=='<2016-06':\n",
    "        ft['properties']['install_date_ints']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson.FeatureCollection(out_fts_trim),open('./results_fcs_v1.3/GB_v1.3.geojson','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make memory efficient for CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dir\n",
    "out_str = 'CN'\n",
    "labels_A = json.load(open('./S2_pics_A/'+out_str+'/labels.json','r')) \n",
    "labels_C = json.load(open('./S2_pics_C/'+out_str+'/labels.json','r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_labels_A = [ll['fname'].split('__')[0].split('/')[-1] for ll in labels_A if ll['label']==0]\n",
    "dn_labels_C = [ll['fname'].split('__')[0].split('/')[-1] for ll in labels_C if ll['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_labels = dn_labels_A+dn_labels_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dn_labels_A), len(labels_A), len(dn_labels_C), len(labels_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = sorted(glob.glob('./results_fcs_v1.0/CN*.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fts = []\n",
    "tracker = 0\n",
    "for f in fnames:\n",
    "    new_fts =json.load(open(f,'r'))['features']\n",
    "    tracker = len(all_fts)\n",
    "    all_fts +=[ft for ft in new_fts if ft['properties']['confidence']=='B' or ft['properties']['S2_ids_0'] not in dn_labels]\n",
    "    print (f, len(new_fts), len(all_fts)-tracker, len(all_fts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson.FeatureCollection(all_fts),open('./results_fcs_v1.1/'+out_str+'_v1.1.geojson','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all A and C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = sorted(glob.glob('./results_fcs_v1.1/*v1.1.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fts = []\n",
    "for f in fnames:\n",
    "    all_fts += [ft for ft in json.load(open(f,'r'))['features'] if ft['properties']['confidence'] in ['A','C']]\n",
    "    print (f, len(all_fts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson.FeatureCollection(all_fts),open('./results_fcs_v1.1/world_c_AC.geojson','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do labels for 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_a =  json.load(open(os.path.join('hand_labelling_client','cnx65a','labels.json'),'r'))\n",
    "labels_b = json.load(open(os.path.join('hand_labelling_client','cnx65b','labels.json'),'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(labels_a+labels_b, open('./hand_labelling_client/cnx65/labels.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consol_fcs = glob.glob('./results_fcs_consolidated/*.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consol_fcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windirs = ['e-e', 'f-f', 'g-g', 'h-h', 'i-i', 'j-j', 'k-k', 'l-l', 'm-m', 'n-n', 'o-o', 'p-p', 'q-q', 'r-r', 's-s', 'v-v', 'x-x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_fts = []\n",
    "for fc in consol_fcs:\n",
    "    fts_fc = json.load(open(fc,'r'))['features']\n",
    "    subdir = fc.split('/')[-1].split('.')[0].lower()\n",
    "    labels =  json.load(open(os.path.join('hand_labelling_client',subdir,'labels.json'),'r'))\n",
    "    \n",
    "    if subdir in ['c-c','d-d']:\n",
    "        for label in labels:\n",
    "            label['fname'] = label['fname'].replace('êž‰',':')\n",
    "    elif subdir in windirs:\n",
    "        for label in labels:\n",
    "            label['fname'] = label['fname'].replace('--','-^').replace('-',':').replace('^','-')\n",
    "    \n",
    "    \n",
    "\n",
    "    T_labels = [ll['fname'].replace('/','\\\\').split('\\\\')[-1].split('__')[0] for ll in labels if ll['label']==1]\n",
    "    #try:\n",
    "    #    print (fts_fc[0]['properties']['primary_id'],T_labels[0])\n",
    "    #except:\n",
    "    #    print ('nuh uh')\n",
    "    new_fts = [ft for ft in fts_fc if ft['properties']['primary_id'] in T_labels]\n",
    "    all_new_fts += new_fts\n",
    "    \n",
    "    print (fc, subdir, len(labels), len(T_labels), len(new_fts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_new_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson.FeatureCollection(all_new_fts),open('./results_fcs_additional/new_C_fts.geojson','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do labels for 1.3 AR et al, KZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ar =  json.load(open(os.path.join('hand_labelling_client','ar-et-al','labels.json'),'r'))\n",
    "labels_kz = json.load(open(os.path.join('hand_labelling_client','kz-add','labels.json'),'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_labels = [ll['fname'].replace('/','\\\\').split('\\\\')[-1].split('__')[0] for ll in labels_ar if ll['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_labels +=[ll['fname'].replace('/','\\\\').split('\\\\')[-1].split('__')[0] for ll in labels_kz if ll['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_fts= json.load(open('./results_fcs_additional/additional_AR_NZ_CD_ID_ET.geojson','r'))['features']\n",
    "kz_fts = json.load(open('./results_fcs_additional/additional_KZ.geojson','r'))['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fts = [ft for ft in ar_fts if ft['properties']['primary_id'] in T_labels]\n",
    "new_fts += [ft for ft in kz_fts if ft['properties']['primary_id'] in T_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson.FeatureCollection(new_fts), open('./results_fcs_additional/filtered_ar_kz.geojson','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
